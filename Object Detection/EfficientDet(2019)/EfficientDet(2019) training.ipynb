{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.dataset import CocoDataset, Resizer, Normalizer, Augmenter, collater\n",
    "from efficientdet.loss import FocalLoss\n",
    "from utils.sync_batchnorm import patch_replication_callback\n",
    "from utils.utils import replace_w_sync_bn, CustomDataParallel, get_last_weights, init_weights, boolean_string\n",
    "\n",
    "import glob\n",
    "from cv2 import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from efficientdet.utils import Anchors\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self, project_file):\n",
    "        self.params = yaml.safe_load(open(project_file).read())\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return self.params.get(item, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithLoss(nn.Module):\n",
    "    def __init__(self, model, debug=False):\n",
    "        super().__init__()\n",
    "        self.criterion = FocalLoss()\n",
    "        self.model = model\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, imgs, annotations, obj_list=None):\n",
    "        _, regression, classification, anchors = self.model(imgs)\n",
    "        if self.debug:\n",
    "            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations,\n",
    "                                                imgs=imgs, obj_list=obj_list)\n",
    "        else:\n",
    "            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations)\n",
    "        return cls_loss, reg_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "def calc_iou(a, b):\n",
    "    # a(anchor) [boxes, (y1, x1, y2, x2)]\n",
    "    # b(gt, coco-style) [boxes, (x1, y1, x2, y2)]\n",
    "\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "    iw = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 0])\n",
    "    ih = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 1])\n",
    "    iw = torch.clamp(iw, min=0)\n",
    "    ih = torch.clamp(ih, min=0)\n",
    "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
    "    ua = torch.clamp(ua, min=1e-8)\n",
    "    intersection = iw * ih\n",
    "    IoU = intersection / ua\n",
    "\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. logo dataset 다운로드 -> datasets 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘datasets’: File exists\n",
      "--2021-10-04 15:56:26--  https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.1/dataset_logo.zip\n",
      "Resolving github.com (github.com)... 15.164.81.167\n",
      "Connecting to github.com (github.com)|15.164.81.167|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/253385242/920dbf00-4122-11eb-8c0a-13d45e9b486b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211004%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211004T065626Z&X-Amz-Expires=300&X-Amz-Signature=7892774d0c7ac797a1e8c17e2d86ff2cfba4ac24b899d28367bef5ad6fa75516&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=253385242&response-content-disposition=attachment%3B%20filename%3Ddataset_logo.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-10-04 15:56:26--  https://github-releases.githubusercontent.com/253385242/920dbf00-4122-11eb-8c0a-13d45e9b486b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211004%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211004T065626Z&X-Amz-Expires=300&X-Amz-Signature=7892774d0c7ac797a1e8c17e2d86ff2cfba4ac24b899d28367bef5ad6fa75516&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=253385242&response-content-disposition=attachment%3B%20filename%3Ddataset_logo.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 78388956 (75M) [application/octet-stream]\n",
      "Saving to: ‘dataset_logo.zip.2’\n",
      "\n",
      "dataset_logo.zip.2  100%[===================>]  74.76M  29.1MB/s    in 2.6s    \n",
      "\n",
      "2021-10-04 15:56:29 (29.1 MB/s) - ‘dataset_logo.zip.2’ saved [78388956/78388956]\n",
      "\n",
      "Archive:  dataset_logo.zip\n",
      "replace datasets/logo/annotations/instances_val.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# download and unzip dataset\n",
    "! mkdir datasets\n",
    "! wget https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.1/dataset_logo.zip\n",
    "! unzip -d datasets/ dataset_logo.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom dataloader 정의 및 model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = Params('projects/birdview_vehicles.yml')\n",
    "params = Params('projects/logo.yml')\n",
    "#params = Params('projects/coco.yml')\n",
    "\n",
    "if params.num_gpus == 0:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "else:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "#opt.saved_path = opt.saved_path + f'/{params.project_name}/'\n",
    "#opt.log_path = opt.log_path + f'/{params.project_name}/tensorboard/'\n",
    "#os.makedirs(opt.log_path, exist_ok=True)\n",
    "#os.makedirs(opt.saved_path, exist_ok=True)\n",
    "\n",
    "training_params = {'batch_size': 16,\n",
    "                   'shuffle': True,\n",
    "                   'drop_last': True,\n",
    "                   'collate_fn': collater,\n",
    "                   'num_workers': 1}\n",
    "\n",
    "val_params = {'batch_size': 1,\n",
    "              'shuffle': False,\n",
    "              'drop_last': True,\n",
    "              'collate_fn': collater,\n",
    "              'num_workers': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "compound_coef = 0\n",
    "\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "training_set = CocoDataset(root_dir=os.path.join('datasets/', params.project_name), set=params.train_set,\n",
    "                           transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "                                                         Augmenter(),\n",
    "                                                         Resizer(input_sizes[compound_coef])]))\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "\n",
    "val_set = CocoDataset(root_dir=os.path.join('datasets/', params.project_name), set=params.val_set,\n",
    "                      transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "                                                    Resizer(input_sizes[compound_coef])]))\n",
    "val_generator = DataLoader(val_set, **val_params)\n",
    "\n",
    "model = EfficientDetBackbone(num_classes=len(params.obj_list), compound_coef=compound_coef,\n",
    "                             ratios=eval(params.anchors_ratios), scales=eval(params.anchors_scales))\n",
    "\n",
    "#model.load_state_dict(torch.load('../../weights/efficientdet-d0.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor, Focal Loss 코드 파헤치기\n",
    "(https://heeyoung-alldata.tistory.com/24 블로그 정리 참고)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_scale = [4., 4., 4., 4., 4., 4., 4., 5., 4.]\n",
    "pyramid_levels = [5, 5, 5, 5, 5, 5, 5, 5, 6]\n",
    "kwargs = {'ratios' : eval(params.anchors_ratios), 'scales' : eval(params.anchors_scales)}\n",
    "\n",
    "anchors = Anchors(anchor_scale=anchor_scale[0],\n",
    "                   pyramid_levels=(torch.arange(pyramid_levels[compound_coef]) + 3).tolist(),\n",
    "                   **kwargs)\n",
    "\n",
    "temp = training_set[330]\n",
    "img = temp['img'].permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "anchor = anchors(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid_level = [3, 4, 5, 6, 7]\n",
    "strides = [2**x for x in pyramid_level]\n",
    "\n",
    "anchors_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "anchors_ratios = [(1.0, 1.0), (1.3, 0.8), (1.9, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxes_all = []\n",
    "for stride in strides:\n",
    "    boxes_level = []\n",
    "    for scale, ratio in itertools.product(anchors_scales, anchors_ratios):\n",
    "        \n",
    "        anchor_scale = 4\n",
    "        base_anchor_size = anchor_scale * stride * scale\n",
    "        anchor_size_x_2 = base_anchor_size * ratio[0] / 2.0\n",
    "        anchor_size_y_2 = base_anchor_size * ratio[1] / 2.0\n",
    "\n",
    "        image_size = 512\n",
    "        \n",
    "        x = np.arange(stride / 2, image_size, stride)\n",
    "        y = np.arange(stride / 2, image_size, stride)\n",
    "        xv, yv = np.meshgrid(x, y)\n",
    "        xv = xv.reshape(-1)\n",
    "        yv = yv.reshape(-1)\n",
    "\n",
    "        # y1,x1,y2,x2\n",
    "        boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n",
    "                           yv + anchor_size_y_2, xv + anchor_size_x_2))\n",
    "        boxes = np.swapaxes(boxes, 0, 1)\n",
    "        boxes_level.append(np.expand_dims(boxes, axis=1))\n",
    "    # concat anchors on the same level to the reshape NxAx4\n",
    "    boxes_level = np.concatenate(boxes_level, axis=1)\n",
    "    boxes_all.append(boxes_level.reshape([-1, 4]))\n",
    "\n",
    "anchor_boxes = np.vstack(boxes_all)\n",
    "\n",
    "anchor_boxes = torch.from_numpy(anchor_boxes.astype(np.float32))\n",
    "anchor_boxes = anchor_boxes.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "compound_coef = 0\n",
    "\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "training_set = CocoDataset(root_dir=os.path.join('datasets/', params.project_name), set=params.train_set,\n",
    "                           transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "                                                         Augmenter(),\n",
    "                                                         Resizer(input_sizes[compound_coef])]))\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "\n",
    "val_set = CocoDataset(root_dir=os.path.join('datasets/', params.project_name), set=params.val_set,\n",
    "                      transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "                                                    Resizer(input_sizes[compound_coef])]))\n",
    "val_generator = DataLoader(val_set, **val_params)\n",
    "\n",
    "model = EfficientDetBackbone(num_classes=len(params.obj_list), compound_coef=compound_coef,\n",
    "                             ratios=eval(params.anchors_ratios), scales=eval(params.anchors_scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = training_set[330]\n",
    "\n",
    "img = temp['img'].permute(2, 0, 1).unsqueeze(0)\n",
    "annotations = temp['annot'].unsqueeze(0)\n",
    "annotations.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, regressions, classifications, anchors = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 49104, 4]),\n",
       " torch.Size([1, 49104, 10]),\n",
       " torch.Size([1, 49104, 4]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressions.size(), classifications.size(), anchors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "batch_size = classifications.shape[0]\n",
    "classification_losses = []\n",
    "regression_losses = []\n",
    "\n",
    "anchor = anchors[0, :, :]  # assuming all image sizes are the same, which it is\n",
    "dtype = anchors.dtype\n",
    "\n",
    "anchor_widths = anchor[:, 3] - anchor[:, 1]        # w\n",
    "anchor_heights = anchor[:, 2] - anchor[:, 0]       # h\n",
    "anchor_ctr_x = anchor[:, 1] + 0.5 * anchor_widths  # center x\n",
    "anchor_ctr_y = anchor[:, 0] + 0.5 * anchor_heights # center y\n",
    "\n",
    "j = 0\n",
    "\n",
    "classification = classifications[j, :, :]                         # torch.Size([49104, 2])\n",
    "regression = regressions[j, :, :]                                 # torch.Size([49104, 4])\n",
    "\n",
    "bbox_annotation = annotations[j]                                  # torch.Size([27, 5])\n",
    "bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]    # torch.Size([27, 5])\n",
    "\n",
    "classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n",
    "\n",
    "\n",
    "IoU = calc_iou(anchor[:, :], bbox_annotation[:, :4])  # (49104, n)\n",
    "\n",
    "IoU_max, IoU_argmax = torch.max(IoU, dim=1)          # IoU_argmax -> 몇 번째 GT bbox와 IoU가 가장 큰지에 대한 index값\n",
    "\n",
    "# compute the loss for classification\n",
    "targets = torch.ones_like(classification) * -1\n",
    "\n",
    "targets[torch.lt(IoU_max, 0.4), :] = 0               # Labeling -> 49104개 중, GT와의 IoU가 0.4보다 작으면 0 부여\n",
    "\n",
    "positive_indices = torch.ge(IoU_max, 0.5)            # Labeling -> 49104개 중, GT와의 IoU가 0.5보다 크면 positive\n",
    "\n",
    "num_positive_anchors = positive_indices.sum()        # 0.5보다 큰게 369개 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_annotations = bbox_annotation[IoU_argmax, :] # 49104개의 anchor에 대응되는 GT 만들어줌(즉, 27 -> 49104개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[positive_indices, :] = 0\n",
    "targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_factor = torch.ones_like(targets) * alpha\n",
    "\n",
    "\n",
    "alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n",
    "focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n",
    "focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
    "\n",
    "bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n",
    "\n",
    "cls_loss = focal_weight * bce\n",
    "\n",
    "zeros = torch.zeros_like(cls_loss)\n",
    "\n",
    "cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)\n",
    "\n",
    "classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.to(dtype), min=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0156,  0.1727,  0.1978, -0.4967],\n",
      "        [-0.0078,  0.0781, -0.4953,  0.1452],\n",
      "        [-0.0098,  0.0601, -0.2722, -0.1172],\n",
      "        [-0.0156,  0.0411,  0.1978, -0.4967],\n",
      "        [-0.0098, -0.1322, -0.2722, -0.1172],\n",
      "        [-0.0156, -0.0905,  0.1978, -0.4967],\n",
      "        [-0.0156, -0.2220,  0.1978, -0.4967]], dtype=torch.float64)\n",
      "tensor([[-0.2777,  0.0536,  0.0441,  1.0189],\n",
      "        [-0.4079,  0.5094, -0.3100,  1.6994],\n",
      "        [ 0.1461,  1.3958,  0.6359, -1.0447],\n",
      "        [-0.1743,  0.7384,  0.6217,  0.7275],\n",
      "        [-0.2631,  0.3858,  0.4581, -0.2207],\n",
      "        [-0.0449, -0.2552,  0.3049,  0.4693],\n",
      "        [-0.4731,  0.1095,  0.4441,  0.1009]], grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "if positive_indices.sum() > 0:\n",
    "    assigned_annotations = assigned_annotations[positive_indices, :]\n",
    "\n",
    "    anchor_widths_pi = anchor_widths[positive_indices]\n",
    "    anchor_heights_pi = anchor_heights[positive_indices]\n",
    "    anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n",
    "    anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n",
    "\n",
    "    gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n",
    "    gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n",
    "    gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    # efficientdet style\n",
    "    gt_widths = torch.clamp(gt_widths, min=1)\n",
    "    gt_heights = torch.clamp(gt_heights, min=1)\n",
    "\n",
    "    targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n",
    "    targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n",
    "    targets_dw = torch.log(gt_widths / anchor_widths_pi)\n",
    "    targets_dh = torch.log(gt_heights / anchor_heights_pi)\n",
    "\n",
    "    targets = torch.stack((targets_dy, targets_dx, targets_dh, targets_dw))\n",
    "    targets = targets.t()\n",
    "\n",
    "    regression_diff = torch.abs(targets - regression[positive_indices, :])\n",
    "\n",
    "    regression_loss = torch.where(torch.le(regression_diff, 1.0 / 9.0), \n",
    "                                  0.5 * 9.0 * torch.pow(regression_diff, 2), \n",
    "                                  regression_diff - 0.5 / 9.0)\n",
    "    regression_losses.append(regression_loss.mean())\n",
    "    \n",
    "print(targets)\n",
    "print(regression[positive_indices, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a66c46fdd674cebb7b57a2630820146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2d12ddd63fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ModelWithLoss(model, debug=False)\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.0001, momentum=0.9, nesterov=True)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    epoch_loss = []\n",
    "    progress_bar = tqdm(training_generator)\n",
    "    for iter, data in enumerate(progress_bar):\n",
    "\n",
    "        imgs = data['img']\n",
    "        annot = data['annot']\n",
    "\n",
    "        if params.num_gpus == 1:\n",
    "            # if only one gpu, just send it to cpu:0\n",
    "            # elif multiple gpus, send it to multiple gpus in CustomDataParallel, not here\n",
    "            imgs = imgs.cpu()\n",
    "            annot = annot.cpu()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n",
    "        cls_loss = cls_loss.mean()\n",
    "        reg_loss = reg_loss.mean()\n",
    "\n",
    "        loss = cls_loss + reg_loss\n",
    "        print(loss)\n",
    "        \n",
    "        if loss == 0 or not torch.isfinite(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
